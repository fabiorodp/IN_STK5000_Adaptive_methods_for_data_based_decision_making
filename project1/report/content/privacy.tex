\section{Privacy Considerations}
\label{sec:privacy}
As a final consideration for this paper, we also discuss the important privacy implications of the data. Our dataset does not contain directly identifying features such as subject name or post code. The data can therefore be considered anonymized, but it is notably not sufficient to guarantee that the individuals described by data could not be identified. The primary identifying features that would be expected to be publicly available are the subjectâ€™s age and gender, as well as their income. Additionally, information regarding where the information was collected could be important to determining the risk to privacy. For instance, if this dataset were collected from the population of the United States, the risk would be relatively low of identifying any individual out of a population of more than $300$ million, using only the age, gender, and income. However, if the dataset is drawn from a relatively smaller population such as the city of Oslo or even a less inhabited area, these features are comparatively more informative for an attacker attempting to identify individuals from the data. This is most likely if, for instance, the attacker knows the population the sample was drawn from, and could have access to other information which could be connected to the data.

One method that could make the data more secure, although less precise, would be to bucket the ages into brackets, rather than the precise value. This would reduce the distinctiveness of any one individual with respect to another when observing the age feature, and potentially establish a notion of $k$-anonymity in the data. \cite{sweeney2002k} This is to say that, for any individual in the data, they will be indistinguishable from at least $k-1$ other individuals in the data (where we are treating all features as quasi-identifiers). As an example, if records are stored with the age feature represented as a bucket, then a 50 year old woman and a 59 year old woman could not be distinguished using this feature. This approach alone may not be likely to properly ensure $k$-anonymity due to the large number of other features, but the age feature may be comparatively more accessible as prior information to a possible attacker (as opposed to e.g. genes or symptoms). Finally, the limitations of $k$-anonymity itself may require alternative approaches (see \citet{machanavajjhala2007diversity} for discussion of so-called \emph{homogeneity attacks} and \emph{background knowledge attacks}, the latter of which relates particularly to the considerations described in the previous paragraph).

As a specific alternative, privacy could also be strongly secured by introducing a differential-privacy algorithm prior to access of the data by a researcher or analyst. In this case, we could define a measure $\epsilon$ of privacy which determines how secure the data will be. For instance, in the collection of statistics over the population, noise can be added to the data prior to the calculation of each statistic. This approach would also be advantageous because any processing of the data done on the output of $\epsilon$-DP will also be $\epsilon$-DP.